Deep Q-Network (DQN) Agent for CartPole-v1
ğŸ“Œ Project Overview

This project implements a Deep Q-Network (DQN) agent from scratch using PyTorch to solve the CartPole-v1 environment from OpenAI Gym (Gymnasium).

The agent learns to balance a pole on a moving cart using reinforcement learning techniques including:

Experience Replay Buffer

Target Network Updates

Epsilon-Greedy Exploration

Multi-seed training for stability analysis

The goal is to reach a high and stable average reward over time.

ğŸ›  Technologies Used

Python 3.x

Gymnasium (CartPole-v1)

PyTorch

NumPy

Matplotlib (for plotting in notebook)

ğŸ“‚ Project Structure
DQN_CARTPOLE/
â”‚
â”œâ”€â”€ saved_models/
â”‚   â”œâ”€â”€ best_seed_42.pth
â”‚   â”œâ”€â”€ best_seed_7.pth
â”‚   â””â”€â”€ best_seed_123.pth
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ agent.py
â”‚   â”œâ”€â”€ model.py
â”‚   â”œâ”€â”€ replay_buffer.py
â”‚   â””â”€â”€ evaluate_dqn.py
â”‚
â”œâ”€â”€ train_dqn.py
â”œâ”€â”€ notebook.ipynb
â”œâ”€â”€ rewards_seed_42.npy
â”œâ”€â”€ rewards_seed_7.npy
â”œâ”€â”€ rewards_seed_123.npy
â””â”€â”€ readme.md

ğŸ§  DQN Implementation Details
1ï¸âƒ£ Neural Network

A fully connected feedforward network is used to approximate Q-values:

Input: 4 state values

Hidden Layers: Fully connected layers with ReLU activation

Output: 2 Q-values (Left / Right actions)

2ï¸âƒ£ Experience Replay

A replay buffer stores past experiences:

(state, action, reward, next_state, done)


Random mini-batches are sampled during training to break correlation between consecutive experiences.

3ï¸âƒ£ Target Network

A separate target network is updated periodically to stabilize learning.

Step-based target update

target_update_steps = 200

4ï¸âƒ£ Epsilon-Greedy Policy

Exploration is controlled using epsilon decay:

Initial epsilon = 1.0

Decay rate = 0.995

Minimum epsilon = 0.01

This balances exploration and exploitation.

5ï¸âƒ£ Reproducibility

To ensure fair evaluation and stability analysis:

Fixed random seeds were used

Models trained on 3 different seeds:

42

7

123

This demonstrates how DQN performance varies with initialization.

ğŸš€ How to Run the Project
1ï¸âƒ£ Install Dependencies
pip install gymnasium[classic-control]
pip install torch
pip install numpy
pip install matplotlib

2ï¸âƒ£ Train the Model

Open train_dqn.py.

Set the seed manually inside the file:

seed = 42


Then run:

python train_dqn.py


To train on other seeds:

Change the seed value

Run the script again

Each run saves:

saved_models/best_seed_<seed>.pth
rewards_seed_<seed>.npy

3ï¸âƒ£ Evaluate a Trained Model

Run:

python src/evaluate_dqn.py


You will be prompted to enter the seed number:

Enter the seed number you want to evaluate:


The agent will run in a visible CartPole environment.

4ï¸âƒ£ Plot Training Results

Open:

notebook.ipynb


The notebook:

Loads reward files

Plots episode rewards

Compares convergence behavior across seeds

Analyzes training stability

ğŸ“Š Observations from Multi-Seed Training

Different seeds showed different convergence speeds.

Some seeds achieved faster stabilization.

Late-stage instability was observed in certain runs.

Reinforcement learning performance is sensitive to initialization and randomness.

This highlights the importance of multi-seed evaluation in reinforcement learning experiments.

ğŸ Final Performance

The agent was able to:

Achieve high episode rewards (close to 500)

Maintain stable performance across multiple evaluation episodes

Demonstrate learning behavior consistent with DQN algorithm

ğŸ¯ Key Takeaways

Target network updates significantly improve stability.

Experience replay reduces variance.

Exploration strategy strongly impacts convergence speed.

Multi-seed training provides more reliable evaluation than single-run results.

ğŸ“š References

Mnih et al., â€œHuman-level control through deep reinforcement learningâ€ (DQN Paper)

OpenAI Gym Documentation

PyTorch Documentation

âœ… Deliverables Included

âœ” train_dqn.py â€“ Training script
âœ” evaluate_dqn.py â€“ Model evaluation script
âœ” notebook.ipynb â€“ Reward visualization and analysis
âœ” Saved trained models
âœ” Reward history files
âœ” This README with complete instructions

ğŸ‘¨â€ğŸ’» Author

Implemented and experimented as part of a reinforcement learning project to understand DQN behavior and training stability in controlled environments.
